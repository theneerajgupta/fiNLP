{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intellectual-oasis",
   "metadata": {},
   "source": [
    "<h2><b>Importing All Required Libraries</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vital-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import nlp\n",
    "import wordninja\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "burning-secretary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Artemis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Artemis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-arizona",
   "metadata": {},
   "source": [
    "<h2><b>Import Datasets</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sophisticated-guitar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWID</th>\n",
       "      <th>NEG</th>\n",
       "      <th>NEU</th>\n",
       "      <th>POS</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768096868504969216</td>\n",
       "      <td>0.049398</td>\n",
       "      <td>0.861395</td>\n",
       "      <td>0.089207</td>\n",
       "      <td>#Incredible #India #Atulya #Bharat - Land of S...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768097619281227776</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.946591</td>\n",
       "      <td>RT @KendallHuntRPD: The #firstdayofschool for ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768097627695042560</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.974948</td>\n",
       "      <td>RT @PEPalerts: This September, @YESmag is taki...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768097640269643776</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.110913</td>\n",
       "      <td>0.876012</td>\n",
       "      <td>RT @BantySrkian: #SRK and kajol in the making ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768097640278089729</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>RT @MianUsmanJaved: Congratulations Pakistan o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TWID       NEG       NEU       POS  \\\n",
       "0  768096868504969216  0.049398  0.861395  0.089207   \n",
       "1  768097619281227776  0.006598  0.046810  0.946591   \n",
       "2  768097627695042560  0.006389  0.018663  0.974948   \n",
       "3  768097640269643776  0.013075  0.110913  0.876012   \n",
       "4  768097640278089729  0.004939  0.029469  0.965591   \n",
       "\n",
       "                                               TWEET SENTIMENT  \n",
       "0  #Incredible #India #Atulya #Bharat - Land of S...   neutral  \n",
       "1  RT @KendallHuntRPD: The #firstdayofschool for ...  positive  \n",
       "2  RT @PEPalerts: This September, @YESmag is taki...  positive  \n",
       "3  RT @BantySrkian: #SRK and kajol in the making ...  positive  \n",
       "4  RT @MianUsmanJaved: Congratulations Pakistan o...  positive  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ds/t4sa-with-text-and-labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "vocal-essence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEG</th>\n",
       "      <th>NEU</th>\n",
       "      <th>POS</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049398</td>\n",
       "      <td>0.861395</td>\n",
       "      <td>0.089207</td>\n",
       "      <td>#Incredible #India #Atulya #Bharat - Land of S...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006598</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.946591</td>\n",
       "      <td>RT @KendallHuntRPD: The #firstdayofschool for ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.974948</td>\n",
       "      <td>RT @PEPalerts: This September, @YESmag is taki...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.110913</td>\n",
       "      <td>0.876012</td>\n",
       "      <td>RT @BantySrkian: #SRK and kajol in the making ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>RT @MianUsmanJaved: Congratulations Pakistan o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NEG       NEU       POS  \\\n",
       "0  0.049398  0.861395  0.089207   \n",
       "1  0.006598  0.046810  0.946591   \n",
       "2  0.006389  0.018663  0.974948   \n",
       "3  0.013075  0.110913  0.876012   \n",
       "4  0.004939  0.029469  0.965591   \n",
       "\n",
       "                                               TWEET SENTIMENT  \n",
       "0  #Incredible #India #Atulya #Bharat - Land of S...   neutral  \n",
       "1  RT @KendallHuntRPD: The #firstdayofschool for ...  positive  \n",
       "2  RT @PEPalerts: This September, @YESmag is taki...  positive  \n",
       "3  RT @BantySrkian: #SRK and kajol in the making ...  positive  \n",
       "4  RT @MianUsmanJaved: Congratulations Pakistan o...  positive  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['TWID']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-botswana",
   "metadata": {},
   "source": [
    "<h2><b>Viterbi Algorithm</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "amazing-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_segment(text):\n",
    "    text.lower()\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words\n",
    "\n",
    "def word_prob(word): return dictionary[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "dictionary = Counter(words(open('ds/big.txt').read()))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "varying-republic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_segment(\"helloworld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-village",
   "metadata": {},
   "source": [
    "<h2><b>Make Functions That Will Perform Preprocessing</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "forty-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unjoin_words(text):\n",
    "    lst = text.split()\n",
    "    sent = []\n",
    "    for x in lst:\n",
    "        sent.append(\" \".join(viterbi_segment(x)))\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "streaming-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unjoin_words(\"hellohoware you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supported-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all hashtags\n",
    "def hashtags(text):\n",
    "    hash = re.findall(r'#(\\w+)', text)\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "relative-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing tweeted at '@username'\n",
    "def remove_users(tweet) :\n",
    "    user = re.sub('@[A-Za-z]+[A-Za-z0-9-_]+', '', tweet)\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "recent-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    url = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contemporary-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use this function if you are doing translation work\n",
    "def non_ascii(s):\n",
    "    return \"\".join(i for i in s if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "willing-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch all words to lowercase\n",
    "def lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "laden-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all stopwords\n",
    "def remove_stopwords(tweet):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # add custom words\n",
    "#     stops.update(('mailto', 'brb', 'it', 'this', 'yolo'))\n",
    "    new_tweet = ' '.join([word for word in tweet.split() if word not in stops])\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expanded-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove email_ids\n",
    "def email_address(tweet):\n",
    "    email = re.compile(r'[\\w\\.-]+@[\\w\\.-]+com')\n",
    "    return email.sub(r'', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "norwegian-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all kind of punctuations\n",
    "def punct(text):\n",
    "    token = RegexpTokenizer(r'\\w+')\n",
    "    text = token.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "muslim-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all numbers\n",
    "def remove_digits(text):\n",
    "    pattern = r'[^a-zA-Z.,!?/:;\\\"\\'\\s]'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elect-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all special characters\n",
    "def remove_special_characters(text):\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pat, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wireless-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation\n",
    "def punct(text):\n",
    "    token = RegexpTokenizer(r'\\w+')\n",
    "    text = token.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "seven-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove RT\n",
    "def remove_rt(text):\n",
    "    return re.sub('RT :', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-unemployment",
   "metadata": {},
   "source": [
    "<h2><b>Negation</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "collective-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for negation\n",
    "givenpatterns = [(r'won\\'t', 'will not'), (r'don\\'t', 'do not'),\n",
    "\t\t\t\t (r'can\\'t', 'can not'), (r'\\'s', 'is'),\n",
    "\t\t\t\t (r'wouldn\\'t', 'would not'), (r'doesn\\'t', 'does not'),\n",
    "\t\t\t\t (r'isn\\'t', 'is not'), (r'shan\\'t', 'shall not'),\n",
    "\t\t\t\t (r'aren\\'t', 'are not'), (r'hasn\\'t', 'has not'),\n",
    "\t\t\t\t (r'wasn\\'t', 'was not'), (r'weren\\'t', 'were not'),\n",
    "\t\t\t\t (r'haven\\'t', 'have not'), (r'hasn\\'t', 'has not'),\n",
    "\t\t\t\t (r'hadn\\'t', 'had not'), (r'didn\\'t', 'did not'),\n",
    "\t\t\t\t (r'couldn\\'t', 'could not'), (r'shouldn\\'t', 'should not'),\n",
    "\t\t\t\t (r'mightn\\'t', 'might not'), (r'mustn\\'t', 'must not'),\n",
    "\t\t\t\t (r'wont', 'will not'), (r'dont', 'do not'), (r\"shalln't\", 'shall not'),\n",
    "\t\t\t\t (r'cant', 'can not'), (r'wouldnt', 'would not'),\n",
    "\t\t\t\t (r'doesnt', 'does not'), (r'isnt', 'is not'),\n",
    "\t\t\t\t (r'shant', 'shall not'), (r'arent', 'are not'),\n",
    "\t\t\t\t (r'hasnt', 'has not'), (r'wasnt', 'was not'),\n",
    "\t\t\t\t (r'werent', 'were not'), (r'havent', 'have not'),\n",
    "\t\t\t\t (r'hasnt', 'has not'), (r'hadnt', 'had not'),\n",
    "\t\t\t\t (r'didnt', 'did not'), (r'couldnt', 'could not'),\n",
    "\t\t\t\t (r'shouldnt', 'should not'), (r'mightnt', 'might not'),\n",
    "\t\t\t\t (r'mustnt', 'must not') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interesting-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntonymReplacer(object):\n",
    "\tdef replace(self, word): #This function is used to find the antonym \n",
    "\t\tantonyms = set()\n",
    "\t\tfor syn in wordnet.synsets(word):\n",
    "\t\t\tfor lemma in syn.lemmas():\n",
    "\t\t\t\tfor antonym in lemma.antonyms():\n",
    "\t\t\t\t\tantonyms.add(antonym.name())\n",
    "\t\tif len(antonyms)== 1:\n",
    "\t\t\treturn antonyms.pop()\n",
    "\t\telse:\n",
    "\t\t\treturn None\n",
    "\n",
    "\n",
    "\tdef negreplace(self, string):# This function is used to evaluate the sentence and find the negation\n",
    "\t\ti=0\n",
    "\t\tsent= word_tokenize(string)\n",
    "\t\tlen_sent = len(sent)\n",
    "\t\t\n",
    "\t\twords= []\n",
    "\t\tfsent = \" \"\n",
    "\t\twhile i<len_sent:\n",
    "\t\t\tword=sent[i]\n",
    "\t\t\t\n",
    "\t\t\tif word == 'not' and i+1 < len_sent:\n",
    "\t\t\t\tant = self.replace(sent[i+1])\n",
    "\t\t\t\tif ant:\n",
    "\t\t\t\t\twords.append(ant)\n",
    "\t\t\t\t\tfsent += ant + \" \"\n",
    "\t\t\t\t\ti = i + 2\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\twords.append(word)\n",
    "\t\t\tfsent += word + \" \"\n",
    "\t\t\ti = i + 1\n",
    "\n",
    "\t\treturn fsent\n",
    "    \n",
    "class RegexReplacer(object): # This is a regular expression function to repalce the words like don't with do not\n",
    "\tdef __init__(self):\n",
    "\t\tself.patterns = givenpatterns\n",
    "\n",
    "\tdef replace(self,text):\n",
    "\t\tfor(raw,rep) in self.patterns:\n",
    "\t\t\tregex = re.compile(raw, re.IGNORECASE)\n",
    "\t\t\ttext = regex.sub(rep, text)\n",
    "\t\treturn text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vertical-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep=AntonymReplacer()\n",
    "reg=RegexReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dedicated-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_negation(text):\n",
    "    jword = reg.replace(text)\n",
    "    final = rep.negreplace(jword)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sought-orientation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I should not shall not that '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_negation(\"I shouldn't shalln't that\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-release",
   "metadata": {},
   "source": [
    "<h2><b>Apply Preprocessing</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "capable-humor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>HASHTAGS</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Incredible #India #Atulya #Bharat - Land of S...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[Incredible, India, Atulya, Bharat, BeProud]</td>\n",
       "      <td>incredible india atulya bharat land seekers be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KendallHuntRPD: The #firstdayofschool for ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[firstdayofschool, EducationMatters]</td>\n",
       "      <td>the firstdayofschool students amp teachers goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @PEPalerts: This September, @YESmag is taki...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[]</td>\n",
       "      <td>this september taking maine mendozas surprise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @BantySrkian: #SRK and kajol in the making ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[SRK]</td>\n",
       "      <td>srk kajol making ddlj song both greatest ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @MianUsmanJaved: Congratulations Pakistan o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[No1TestTeam, JI_PakZindabadRallies]</td>\n",
       "      <td>congratulations pakistan becoming notestteam w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TWEET SENTIMENT  \\\n",
       "0  #Incredible #India #Atulya #Bharat - Land of S...   neutral   \n",
       "1  RT @KendallHuntRPD: The #firstdayofschool for ...  positive   \n",
       "2  RT @PEPalerts: This September, @YESmag is taki...  positive   \n",
       "3  RT @BantySrkian: #SRK and kajol in the making ...  positive   \n",
       "4  RT @MianUsmanJaved: Congratulations Pakistan o...  positive   \n",
       "\n",
       "                                       HASHTAGS  \\\n",
       "0  [Incredible, India, Atulya, Bharat, BeProud]   \n",
       "1          [firstdayofschool, EducationMatters]   \n",
       "2                                            []   \n",
       "3                                         [SRK]   \n",
       "4          [No1TestTeam, JI_PakZindabadRallies]   \n",
       "\n",
       "                                                TEXT  \n",
       "0  incredible india atulya bharat land seekers be...  \n",
       "1  the firstdayofschool students amp teachers goo...  \n",
       "2  this september taking maine mendozas surprise ...  \n",
       "3      srk kajol making ddlj song both greatest ever  \n",
       "4  congratulations pakistan becoming notestteam w...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['HASHTAGS'] = df.TWEET.apply(func = hashtags)\n",
    "df['TEXT'] = df.TWEET.apply(func = remove_users)\n",
    "df['TEXT'] = df.TEXT.apply(func = remove_links)\n",
    "df['TEXT'] = df.TEXT.apply(func = remove_rt)\n",
    "df['TEXT'] = df.TEXT.apply(func = non_ascii)\n",
    "df['TEXT'] = df.TEXT.apply(func = remove_stopwords)\n",
    "df['TEXT'] = df.TEXT.apply(func = remove_digits)\n",
    "df['TEXT'] = df.TEXT.apply(func = remove_special_characters)\n",
    "df['TEXT'] = df.TEXT.apply(func = punct)\n",
    "df['TEXT'] = df.TEXT.apply(func = email_address)\n",
    "# df['TEXT'] = df.TEXT.apply(func = unjoin_words)\n",
    "df['TEXT'] = df.TEXT.apply(func = lower)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bright-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"ds/t4sa-preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "romantic-cardiff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>HASHTAGS</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1736</td>\n",
       "      <td>1736</td>\n",
       "      <td>1736</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1585</td>\n",
       "      <td>3</td>\n",
       "      <td>611</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>RT @allkpop: GOT7 look back on 115 days of 'Fl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[]</td>\n",
       "      <td>got look back days fly tour special mv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>20</td>\n",
       "      <td>1155</td>\n",
       "      <td>977</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    TWEET SENTIMENT HASHTAGS  \\\n",
       "count                                                1736      1736     1736   \n",
       "unique                                               1585         3      611   \n",
       "top     RT @allkpop: GOT7 look back on 115 days of 'Fl...   neutral       []   \n",
       "freq                                                   20      1155      977   \n",
       "\n",
       "                                          TEXT  \n",
       "count                                     1736  \n",
       "unique                                    1524  \n",
       "top     got look back days fly tour special mv  \n",
       "freq                                        20  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "respected-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1155\n",
       "positive     531\n",
       "negative      50\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SENTIMENT'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
